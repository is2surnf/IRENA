# app/services/ia.py
from typing import Iterable, List, Dict
import google.generativeai as genai
from app.core.config import settings
import logging
import json

logger = logging.getLogger(__name__)

# Configurar Gemini
if settings.GEMINI_API_KEY:
    try:
        genai.configure(api_key=settings.GEMINI_API_KEY)
        logger.info("‚úÖ Google Gemini configurado correctamente")
    except Exception as e:
        logger.error(f"‚ùå Error configurando Gemini: {e}")

SYSTEM_PROMPT = (
    "Eres 'Asistente Qu√≠mico' de IReNaTech. Respondes en espa√±ol neutro.\n"
    "- Especialista en qu√≠mica (general, org√°nica, inorg√°nica, anal√≠tica y fisicoqu√≠mica).\n"
    "- Usa unidades SI, advierte riesgos y buenas pr√°cticas de laboratorio.\n"
    "- Balancea ecuaciones cuando sea necesario y explica pasos.\n"
    "- Mant√©n un tono profesional pero accesible.\n"
    "- Si no sabes algo, adm√≠telo y sugiere recursos adicionales."
)

def _no_key_reply() -> str:
    return (
        "‚ö†Ô∏è **Error de Configuraci√≥n**\n\n"
        "El servidor no tiene configurada la variable GEMINI_API_KEY.\n"
        "Por favor, contacta al administrador para que:\n"
        "1. Agregue la API key en el archivo .env\n"
        "2. Reinicie el servidor backend\n\n"
        "Mientras tanto, puedes revisar la documentaci√≥n en /docs"
    )

def _connection_error_reply(error: str) -> str:
    return (
        f"‚ö†Ô∏è **Error de Conexi√≥n**\n\n"
        f"No se pudo conectar con el servicio de Google Gemini:\n"
        f"`{error}`\n\n"
        "Esto puede deberse a:\n"
        "- Problemas de red\n"
        "- API key inv√°lida o expirada\n"
        "- L√≠mite de uso alcanzado\n\n"
        "Por favor, int√©ntalo de nuevo en unos momentos."
    )

def _convert_messages_to_gemini(messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """
    Convierte el formato de mensajes de OpenAI al formato de Gemini.
    Gemini usa 'user' y 'model' en lugar de 'user' y 'assistant'.
    """
    gemini_messages = []
    
    for msg in messages:
        role = msg["role"]
        content = msg["content"]
        
        # Ignoramos el mensaje del sistema aqu√≠ porque se maneja con `system_instruction`
        if role == "user":
            gemini_messages.append({
                "role": "user", 
                "parts": [{"text": content}]
            })
        elif role == "assistant":
            gemini_messages.append({
                "role": "model",
                "parts": [{"text": content}]
            })
    
    return gemini_messages

def chat_completion(messages: List[Dict[str, str]], model: str = None) -> str:
    """
    Genera una respuesta completa usando Google Gemini
    """
    if not settings.GEMINI_API_KEY:
        logger.warning("üö® Intento de usar IA sin API key configurada")
        return _no_key_reply()
    
    try:
        logger.info(f"ü§ñ Enviando {len(messages)} mensajes a Gemini (modelo: {settings.GEMINI_MODEL})")
        
        # Inicializar el modelo con el system_instruction
        gemini_model = genai.GenerativeModel(
            model_name=settings.GEMINI_MODEL,
            system_instruction=SYSTEM_PROMPT
        )
        
        # Convertir mensajes al formato de Gemini
        gemini_messages = _convert_messages_to_gemini(messages)
        
        # Si hay mensajes de conversaci√≥n, usamos chat
        if len(gemini_messages) > 1:
            # Crear chat con historial
            chat = gemini_model.start_chat(history=gemini_messages[:-1])
            
            # Obtener el √∫ltimo mensaje del usuario
            last_message = gemini_messages[-1]
            if last_message["role"] == "user":
                response = chat.send_message(
                    last_message["parts"][0]["text"],
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.2,
                        max_output_tokens=2000,
                        candidate_count=1,
                    )
                )
            else:
                # Fallback: generar respuesta directa
                response = gemini_model.generate_content(
                    last_message["parts"][0]["text"],
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.2,
                        max_output_tokens=2000,
                        candidate_count=1,
                    )
                )
        else:
            # Solo un mensaje, generar respuesta directa
            user_message = gemini_messages[0]["parts"][0]["text"] if gemini_messages else "Hola"
            response = gemini_model.generate_content(
                user_message,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.2,
                    max_output_tokens=2000,
                    candidate_count=1,
                )
            )
        
        reply = response.text if response.text else ""
        
        if not reply:
            logger.warning("‚ö†Ô∏è Gemini devolvi√≥ una respuesta vac√≠a")
            return "Lo siento, no pude generar una respuesta. Por favor, int√©ntalo de nuevo."
        
        logger.info(f"‚úÖ Respuesta recibida de Gemini: {len(reply)} caracteres")
        return reply
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"‚ùå Error en chat_completion: {error_msg}", exc_info=True)
        return _connection_error_reply(error_msg)

def chat_stream(messages: List[Dict[str, str]], model: str = None) -> Iterable[str]:
    """
    Genera chunks de texto para streaming usando Google Gemini
    """
    if not settings.GEMINI_API_KEY:
        logger.warning("üö® Intento de usar streaming sin API key configurada")
        yield _no_key_reply()
        yield "[DONE]"
        return

    try:
        logger.info(f"üì° Iniciando streaming con {len(messages)} mensajes (modelo: {settings.GEMINI_MODEL})")
        
        # Inicializar el modelo con el system_instruction
        gemini_model = genai.GenerativeModel(
            model_name=settings.GEMINI_MODEL,
            system_instruction=SYSTEM_PROMPT
        )
        
        # Convertir mensajes al formato de Gemini
        gemini_messages = _convert_messages_to_gemini(messages)
        
        # Preparar el contenido para streaming
        if len(gemini_messages) > 1:
            # Crear chat con historial
            chat = gemini_model.start_chat(history=gemini_messages[:-1])
            last_message = gemini_messages[-1]
            
            if last_message["role"] == "user":
                response = chat.send_message(
                    last_message["parts"][0]["text"],
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.2,
                        max_output_tokens=2000,
                        candidate_count=1,
                    ),
                    stream=True
                )
            else:
                response = gemini_model.generate_content(
                    last_message["parts"][0]["text"],
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.2,
                        max_output_tokens=2000,
                        candidate_count=1,
                    ),
                    stream=True
                )
        else:
            # Solo un mensaje
            user_message = gemini_messages[0]["parts"][0]["text"] if gemini_messages else "Hola"
            response = gemini_model.generate_content(
                user_message,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.2,
                    max_output_tokens=2000,
                    candidate_count=1,
                ),
                stream=True
            )
        
        chunk_count = 0
        for chunk in response:
            if chunk.text:
                chunk_count += 1
                yield chunk.text
        
        logger.info(f"‚úÖ Streaming completado: {chunk_count} chunks enviados")
        yield "[DONE]"
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"‚ùå Error en chat_stream: {error_msg}", exc_info=True)
        yield _connection_error_reply(error_msg)
        yield "[DONE]"

# Funci√≥n auxiliar para verificar la configuraci√≥n de Gemini
def test_gemini_connection() -> Dict:
    """
    Prueba la conexi√≥n con Gemini y devuelve informaci√≥n del estado
    """
    if not settings.GEMINI_API_KEY:
        return {
            "status": "error",
            "message": "API key no configurada",
            "gemini_configured": False
        }
    
    try:
        # Probar una consulta simple
        model = genai.GenerativeModel(
            model_name=settings.GEMINI_MODEL,
            system_instruction=SYSTEM_PROMPT
        )
        response = model.generate_content(
            "Responde solo 'OK' si me recibes",
            generation_config=genai.types.GenerationConfig(
                temperature=0.1,
                max_output_tokens=10,
                candidate_count=1,
            )
        )
        
        return {
            "status": "ok",
            "message": "Conexi√≥n exitosa con Gemini",
            "gemini_configured": True,
            "model": settings.GEMINI_MODEL,
            "test_response": response.text[:50] if response.text else "Sin respuesta"
        }
    
    except Exception as e:
        return {
            "status": "error", 
            "message": f"Error de conexi√≥n: {str(e)}",
            "gemini_configured": False
        }